{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"word_based_encodings.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNrM+96Vf1/bUZJRlncZyVb"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vZg-1NN5mstz","executionInfo":{"status":"ok","timestamp":1650226105182,"user_tz":-360,"elapsed":1530,"user":{"displayName":"S.M.ABRAR MUSTAKIM TAKI","userId":"04264818990090801992"}},"outputId":"87f202d0-24d5-46bb-82b0-1ae41b93cee5"},"outputs":[{"output_type":"stream","name":"stdout","text":["{'love': 1, 'my': 2, 'i': 3, 'dog': 4, 'cat': 5, 'you': 6}\n"]}],"source":["import tensorflow as tf\n","from tensorflow import keras \n","from tensorflow.keras.preprocessing.text import Tokenizer\n","\n","sentence = [\"I love my dog\",\n","            \"I love my cat\",\n","            \"You love my dog!\"]\n","\n","tokenizer = Tokenizer(num_words=150)\n","\n","tokenizer.fit_on_texts(sentence)\n","\n","word_index = tokenizer.word_index \n","\n","print(word_index)\n"]},{"cell_type":"code","source":["sequences = tokenizer.texts_to_sequences(sentence)\n","print(sequences)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"A9XbhxcGrU3w","executionInfo":{"status":"ok","timestamp":1650226106706,"user_tz":-360,"elapsed":27,"user":{"displayName":"S.M.ABRAR MUSTAKIM TAKI","userId":"04264818990090801992"}},"outputId":"20e3d923-5d20-4416-97d7-09fe94093f74"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[3, 1, 2, 4], [3, 1, 2, 5], [6, 1, 2, 4]]\n"]}]},{"cell_type":"code","source":["test_data = [\"I love my Laptop\",\n","             \"My dog loves my cat\"]\n","\n","test_seq = tokenizer.texts_to_sequences(test_data)\n","print(test_seq)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pJ0WKRY7rU6r","executionInfo":{"status":"ok","timestamp":1650226106707,"user_tz":-360,"elapsed":24,"user":{"displayName":"S.M.ABRAR MUSTAKIM TAKI","userId":"04264818990090801992"}},"outputId":"42db9347-8113-4fc1-80b8-ad9ba92ada74"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[3, 1, 2], [2, 4, 2, 5]]\n"]}]},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow import keras \n","from tensorflow.keras.preprocessing.text import Tokenizer\n","\n","sentence = [\"I love my dog\",\n","            \"I love my cat\",\n","            \"You love my dog!\"]\n","\n","tokenizer = Tokenizer(num_words=150, oov_token=\"<OOV>\")\n","\n","tokenizer.fit_on_texts(sentence)\n","\n","word_index = tokenizer.word_index \n","\n","print(word_index)\n","\n","sequences = tokenizer.texts_to_sequences(sentence)\n","print(sequences)\n","\n","\n","test_data = [\"I love my Laptop\",\n","             \"My dog loves my cat\"]\n","\n","test_seq = tokenizer.texts_to_sequences(test_data)\n","print(test_seq)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zfIQGewesIeN","executionInfo":{"status":"ok","timestamp":1650226106707,"user_tz":-360,"elapsed":18,"user":{"displayName":"S.M.ABRAR MUSTAKIM TAKI","userId":"04264818990090801992"}},"outputId":"75ae9918-9fe7-4b1f-f4ac-00b307d4813a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'<OOV>': 1, 'love': 2, 'my': 3, 'i': 4, 'dog': 5, 'cat': 6, 'you': 7}\n","[[4, 2, 3, 5], [4, 2, 3, 6], [7, 2, 3, 5]]\n","[[4, 2, 3, 1], [3, 5, 1, 3, 6]]\n"]}]},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow import keras \n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","sentence = [\"I love my dog\",\n","            \"I love my cat\",\n","            \"You love my dog!\"]\n","\n","tokenizer = Tokenizer(num_words=150, oov_token=\"<OOV>\")\n","\n","tokenizer.fit_on_texts(sentence)\n","\n","word_index = tokenizer.word_index \n","\n","print(word_index)\n","\n","sequences = tokenizer.texts_to_sequences(sentence)\n","print(sequences)\n","\n","padded = pad_sequences(sequences)\n","print(padded)\n","\n","test_data = [\"I love my Laptop\",\n","             \"My dog loves my cat\"]\n","\n","test_seq = tokenizer.texts_to_sequences(test_data)\n","print(test_seq)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ON5w_-H5wyiz","executionInfo":{"status":"ok","timestamp":1650226106709,"user_tz":-360,"elapsed":16,"user":{"displayName":"S.M.ABRAR MUSTAKIM TAKI","userId":"04264818990090801992"}},"outputId":"1143f51d-3ff7-40ca-fb97-e0cf66dbf159"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'<OOV>': 1, 'love': 2, 'my': 3, 'i': 4, 'dog': 5, 'cat': 6, 'you': 7}\n","[[4, 2, 3, 5], [4, 2, 3, 6], [7, 2, 3, 5]]\n","[[4 2 3 5]\n"," [4 2 3 6]\n"," [7 2 3 5]]\n","[[4, 2, 3, 1], [3, 5, 1, 3, 6]]\n"]}]},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow import keras \n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","sentence = [\"I love my dog\",\n","            \"I love my cat\",\n","            \"You love my dog!\"]\n","\n","tokenizer = Tokenizer(num_words=150, oov_token=\"<OOV>\")\n","\n","tokenizer.fit_on_texts(sentence)\n","\n","word_index = tokenizer.word_index \n","\n","print(word_index)\n","\n","sequences = tokenizer.texts_to_sequences(sentence)\n","print(sequences)\n","\n","padded = pad_sequences(sequences, padding=\"post\", truncating=\"post\", maxlen=6)\n","print(padded)\n","\n","test_data = [\"I love my Laptop\",\n","             \"My dog loves my cat\"]\n","\n","test_seq = tokenizer.texts_to_sequences(test_data)\n","print(test_seq)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9o1XL78Z1ema","executionInfo":{"status":"ok","timestamp":1650290501787,"user_tz":-360,"elapsed":3034,"user":{"displayName":"S.M.ABRAR MUSTAKIM TAKI","userId":"04264818990090801992"}},"outputId":"ecdb40c8-d9a8-491c-803c-58a9c0ab0136"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["{'<OOV>': 1, 'love': 2, 'my': 3, 'i': 4, 'dog': 5, 'cat': 6, 'you': 7}\n","[[4, 2, 3, 5], [4, 2, 3, 6], [7, 2, 3, 5]]\n","[[4 2 3 5 0 0]\n"," [4 2 3 6 0 0]\n"," [7 2 3 5 0 0]]\n","[[4, 2, 3, 1], [3, 5, 1, 3, 6]]\n"]}]},{"cell_type":"code","source":["#https://www.kaggle.com/datasets/rmisra/news-headlines-dataset-for-sarcasm-detection"],"metadata":{"id":"1n4b5JRO1ev3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[""],"metadata":{"id":"Ms6o_NlBOkiT"}},{"cell_type":"markdown","source":["<h1>Sarcasm Detector</h1>"],"metadata":{"id":"MSb2KKZIOklS"}},{"cell_type":"code","source":["#Datasets\n","!wget https://storage.googleapis.com/tensorflow-1-public/course3/sarcasm.json"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z3IC50-DOeU0","executionInfo":{"status":"ok","timestamp":1650290489375,"user_tz":-360,"elapsed":17,"user":{"displayName":"S.M.ABRAR MUSTAKIM TAKI","userId":"04264818990090801992"}},"outputId":"5768c64e-0662-4398-cac7-15e8f1ee1b54"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["--2022-04-18 14:01:29--  https://storage.googleapis.com/tensorflow-1-public/course3/sarcasm.json\n","Resolving storage.googleapis.com (storage.googleapis.com)... 64.233.189.128, 108.177.97.128, 108.177.125.128, ...\n","Connecting to storage.googleapis.com (storage.googleapis.com)|64.233.189.128|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 5643545 (5.4M) [application/json]\n","Saving to: ‘sarcasm.json’\n","\n","\rsarcasm.json          0%[                    ]       0  --.-KB/s               \rsarcasm.json        100%[===================>]   5.38M  --.-KB/s    in 0.05s   \n","\n","2022-04-18 14:01:29 (115 MB/s) - ‘sarcasm.json’ saved [5643545/5643545]\n","\n"]}]},{"cell_type":"code","source":["import json \n","\n","with open(\"sarcasm.json\", \"r\") as f:\n","  datastore = json.load(f)\n","\n","sentences = []\n","labels = []\n","urls = []\n","\n","for item in datastore: \n","  sentences.append(item[\"headline\"])\n","  labels.append(item[\"is_sarcastic\"])\n","  urls.append(item[\"article_link\"])\n","\n","\n","tokenizers = Tokenizer(num_words=1000, oov_token=\"<OOV>\")\n","\n","tokenizers.fit_on_texts(sentences)\n","\n","word_index = tokenizers.word_index\n","\n","print(word_index)\n","\n","sequences = tokenizers.texts_to_sequences(sentences)\n","\n","padding = pad_sequences(sequences)\n","\n","print(padding[0]) \n","\n","print(padding.shape)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":235},"id":"aoQHovGLOeYF","executionInfo":{"status":"error","timestamp":1650290493501,"user_tz":-360,"elapsed":22,"user":{"displayName":"S.M.ABRAR MUSTAKIM TAKI","userId":"04264818990090801992"}},"outputId":"3209172f-7fc0-4f2a-f9cc-91e07ca59b5e"},"execution_count":3,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-026a48d71dc9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mtokenizers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moov_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"<OOV>\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mtokenizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_on_texts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'Tokenizer' is not defined"]}]}]}